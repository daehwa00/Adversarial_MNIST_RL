{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The blackcellmagic extension is already loaded. To reload it, use:\n",
            "  %reload_ext blackcellmagic\n"
          ]
        }
      ],
      "source": [
        "%load_ext blackcellmagic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current cuda device is cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "is_cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if is_cuda else 'cpu')\n",
        "\n",
        "print('Current cuda device is', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CNN Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "batch_size = 3000\n",
        "learning_rate = 0.005\n",
        "epoch_num = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load MNIST dataset\n",
        "train_data = datasets.MNIST(root = './MNIST_data/',\n",
        "                            train=True,\n",
        "                            download=True,\n",
        "                            transform=transforms.ToTensor())\n",
        "\n",
        "test_data = datasets.MNIST(root = './MNIST_data/',\n",
        "                            train=False,\n",
        "                            download=True,\n",
        "                            transform=transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x249c3e9eeb0>"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "image, label = train_data[0]\n",
        "shapes = image.squeeze().shape\n",
        "\n",
        "center = np.random.randint(1,(shapes[0]-2),2) # 1에서 26사이 2개\n",
        "\n",
        "image = image.squeeze().numpy()\n",
        "\n",
        "image[center[0],center[1]] = 1  # 점 찍기\n",
        "\n",
        "plt.imshow(image, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset=train_data, batch_size=batch_size, shuffle=True\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset=test_data, batch_size=batch_size, shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, padding_mode, dilation, groups, bias)\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=1,\n",
        "            out_channels=32,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=\"same\",\n",
        "        )\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1, padding=\"same\")\n",
        "        self.dropout = nn.Dropout2d(0.25)\n",
        "        self.fc1 = nn.Linear(3136, 1000)  # 7 * 7 * 64 = 3136\n",
        "        self.fc2 = nn.Linear(1000, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim = 1)\n",
        "\n",
        "\n",
        "classification_model = CNN().to(device)\n",
        "optimizer = optim.Adam(classification_model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 0 \tLoss : 2.174981\n",
            "Epoch : 1 \tLoss : 2.147637\n",
            "Epoch : 2 \tLoss : 1.793685\n",
            "Epoch : 3 \tLoss : 1.775841\n",
            "Epoch : 4 \tLoss : 1.759803\n",
            "Epoch : 5 \tLoss : 1.757428\n",
            "Epoch : 6 \tLoss : 1.758481\n",
            "Epoch : 7 \tLoss : 1.749619\n",
            "Epoch : 8 \tLoss : 1.744121\n",
            "Epoch : 9 \tLoss : 1.667704\n",
            "Epoch : 10 \tLoss : 1.657732\n",
            "Epoch : 11 \tLoss : 1.658432\n",
            "Epoch : 12 \tLoss : 1.656346\n",
            "Epoch : 13 \tLoss : 1.658827\n",
            "Epoch : 14 \tLoss : 1.647263\n",
            "Epoch : 15 \tLoss : 1.637115\n",
            "Epoch : 16 \tLoss : 1.569972\n",
            "Epoch : 17 \tLoss : 1.567602\n",
            "Epoch : 18 \tLoss : 1.550052\n",
            "Epoch : 19 \tLoss : 1.475710\n",
            "Epoch : 20 \tLoss : 1.473871\n",
            "Epoch : 21 \tLoss : 1.469015\n",
            "Epoch : 22 \tLoss : 1.471248\n",
            "Epoch : 23 \tLoss : 1.467474\n",
            "Epoch : 24 \tLoss : 1.467254\n",
            "Epoch : 25 \tLoss : 1.471229\n",
            "Epoch : 26 \tLoss : 1.467386\n",
            "Epoch : 27 \tLoss : 1.465493\n",
            "Epoch : 28 \tLoss : 1.465832\n",
            "Epoch : 29 \tLoss : 1.466300\n"
          ]
        }
      ],
      "source": [
        "classification_model.train()    # train mode\n",
        "for epoch in range(epoch_num):\n",
        "    for data, target in train_loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = classification_model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    #epoch마다 loss 출력\n",
        "    print('Epoch : {} \\tLoss : {:.6f}'.format(epoch, loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set Accuracy : 99.21%\n"
          ]
        }
      ],
      "source": [
        "classification_model.eval()    # 평가시에는 dropout이 OFF 된다.\n",
        "\n",
        "correct = 0\n",
        "for data, target in test_loader:\n",
        "    data = data.to(device)\n",
        "    target = target.to(device)\n",
        "    output = classification_model(data)\n",
        "    prediction = output.data.max(1)[1]\n",
        "    correct += prediction.eq(target.data).sum()\n",
        "print('Test set Accuracy : {:.2f}%'.format(100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CNN 모델 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(classification_model.state_dict(), './model/classification_model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CNN 모델 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, padding_mode, dilation, groups, bias)\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=1,\n",
        "            out_channels=32,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=\"same\",\n",
        "        )\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1, padding=\"same\")\n",
        "        self.dropout = nn.Dropout2d(0.25)\n",
        "        self.fc1 = nn.Linear(3136, 1000)  # 7 * 7 * 64 = 3136\n",
        "        self.fc2 = nn.Linear(1000, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim = 1)\n",
        "\n",
        "\n",
        "classification_model = CNN().to(device)\n",
        "classification_model.load_state_dict(torch.load('./model/classification_model.pt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MNIST Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class MnistEnv():\n",
        "    def __init__(self, classification_model=classification_model, random=True):\n",
        "        super().__init__()\n",
        "        self.classification_model = classification_model  # pre-trained CNN model\n",
        "\n",
        "    def step(self, original_image, action):  # action = [위치, 밝기]\n",
        "        action = torch.tensor(action)\n",
        "        action = torch.sigmoid(action)\n",
        "\n",
        "        point = (action[:, 0] * 676).int()\n",
        "        brightness = ((action[:, 1] * 255).int()) / 255\n",
        "\n",
        "        arr = []\n",
        "        center = [torch.div(point, 26, rounding_mode=\"trunc\") + 1, point % 26 + 1]\n",
        "        action = [point, brightness]\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            changed_image = original_image[i].squeeze().squeeze().cpu().numpy()  #   [28,28]\n",
        "            # Stamp 찍기\n",
        "            changed_image[center[0][i], center[1][i]] = brightness[i]\n",
        "            arr.append(changed_image)\n",
        "\n",
        "        changed_images = torch.stack([torch.tensor(a).unsqueeze(0) for a in arr], dim=0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            original_outputs = self.classification_model(original_image.to(device))\n",
        "            changed_outputs = self.classification_model(changed_images.to(device))\n",
        "\n",
        "        rewards = torch.sum(\n",
        "            torch.nn.functional.kl_div(\n",
        "                original_outputs.log(), changed_outputs, size_average = None, reduction=\"none\"\n",
        "            ),\n",
        "            dim=1,\n",
        "        )\n",
        "\n",
        "        # original_outputs와 changed_outputs의 max의 index가 같으면 reward를 -1/reward로 준다.\n",
        "        rewards[original_outputs.max(1)[1] == changed_outputs.max(1)[1]] = (\n",
        "            -1 / rewards[original_outputs.max(1)[1] == changed_outputs.max(1)[1]]\n",
        "        )\n",
        "\n",
        "        rewards[rewards < -1000] = -1000\n",
        "        rewards[rewards > 1000] = 1000\n",
        "\n",
        "        rewards = torch.nan_to_num(rewards, nan=0.0, posinf=1000, neginf=-1000) # nan을 0으로, inf를 100으로 바꾼다.\n",
        "\n",
        "        return rewards.cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RL model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "num_seeds = 10\n",
        "action_size = 2\n",
        "RL_epoch = 100\n",
        "\n",
        "\n",
        "env_name = \"MnistEnv-v1\"\n",
        "agent_name = \"REINFORCE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from math import floor\n",
        "from typing import Tuple\n",
        "import random\n",
        "\n",
        "RL_learning_rate = 0.00000000001\n",
        "\n",
        "\n",
        "class REINFORCE(nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        super(REINFORCE, self).__init__()\n",
        "\n",
        "        self.data = []\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, 1, padding=\"same\")\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1, padding=\"same\")\n",
        "\n",
        "        self.fc1 = nn.Linear(3136, 1024)  # 7 * 7 * 64 = 3136\n",
        "        self.fc2 = nn.Linear(1024, 1024)\n",
        "\n",
        "        self.fc_mu = torch.nn.Linear(1024, action_size)\n",
        "        self.fc_std = torch.nn.Linear(1024, action_size)\n",
        "\n",
        "        self.dropout = nn.Dropout2d(0.25)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=RL_learning_rate)\n",
        "\n",
        "    def forward(self, state):\n",
        "        state = self.conv1(state)  # conv1\n",
        "        state = F.relu(state)\n",
        "        state = F.max_pool2d(state, 2)\n",
        "        state = self.conv2(state)  # conv2\n",
        "        state = F.relu(state)\n",
        "        state = F.max_pool2d(state, 2)\n",
        "        state = self.dropout(state)\n",
        "        state = torch.flatten(state, 1)\n",
        "        state = self.fc1(state)  # fc1\n",
        "        state = F.relu(state)\n",
        "        state = self.fc2(state)  # fc2\n",
        "        state = F.relu(state)\n",
        "        mu = self.fc_mu(state)\n",
        "        std = self.fc_std(state)\n",
        "\n",
        "        # sigmoid를 통해 0~1사이의 값으로 만들어주며, softplus를 통해 0~무한대의 값으로 만들어준다.\n",
        "        return torch.sigmoid(mu), F.softplus(std)\n",
        "\n",
        "    def put_data(self, transition):\n",
        "        self.data.append(transition)\n",
        "\n",
        "    def train_net(self):\n",
        "        self.optimizer.zero_grad()\n",
        "        r_lst, log_prob_lst = [], []\n",
        "        for transition in self.data:\n",
        "            r_lst.append(transition[0])\n",
        "            log_prob_lst.append(transition[1])\n",
        "        r_lst = torch.tensor(r_lst).to(device)\n",
        "        log_prob_lst = torch.stack(log_prob_lst).to(device)\n",
        "        log_prob_lst = (-1) * log_prob_lst\n",
        "        loss = log_prob_lst * r_lst\n",
        "        loss.mean().backward()\n",
        "        self.optimizer.step()\n",
        "        self.data = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def seed_all(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed) # manual_seed는 CPU에 사용되는 난수 생성기의 시드를 설정합니다.\n",
        "    torch.cuda.manual_seed(seed) # cuda.manual_seed는 GPU에 사용되는 난수 생성기의 시드를 설정합니다.\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True # cudnn.deterministic은 cudnn의 결과를 동일하게 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Start\n",
            "epoch : 1 / 100 | epoch_score : -101.04\n",
            "epoch : 2 / 100 | epoch_score : -96.40\n",
            "epoch : 3 / 100 | epoch_score : -113.06\n",
            "epoch : 4 / 100 | epoch_score : -101.92\n",
            "epoch : 5 / 100 | epoch_score : -109.76\n",
            "epoch : 6 / 100 | epoch_score : -106.55\n",
            "epoch : 7 / 100 | epoch_score : -106.02\n",
            "epoch : 8 / 100 | epoch_score : -101.98\n",
            "epoch : 9 / 100 | epoch_score : -95.96\n",
            "epoch : 10 / 100 | epoch_score : -98.10\n",
            "epoch : 11 / 100 | epoch_score : -98.96\n",
            "epoch : 12 / 100 | epoch_score : -102.98\n",
            "epoch : 13 / 100 | epoch_score : -103.32\n",
            "epoch : 14 / 100 | epoch_score : -99.19\n",
            "epoch : 15 / 100 | epoch_score : -103.82\n",
            "epoch : 16 / 100 | epoch_score : -105.68\n",
            "epoch : 17 / 100 | epoch_score : -100.93\n",
            "epoch : 18 / 100 | epoch_score : -108.20\n",
            "epoch : 19 / 100 | epoch_score : -97.28\n",
            "epoch : 20 / 100 | epoch_score : -96.82\n",
            "epoch : 21 / 100 | epoch_score : -99.75\n",
            "epoch : 22 / 100 | epoch_score : -100.26\n",
            "epoch : 23 / 100 | epoch_score : -99.47\n",
            "epoch : 24 / 100 | epoch_score : -101.11\n",
            "epoch : 25 / 100 | epoch_score : -99.90\n",
            "epoch : 26 / 100 | epoch_score : -103.67\n",
            "epoch : 27 / 100 | epoch_score : -104.90\n",
            "epoch : 28 / 100 | epoch_score : -101.09\n",
            "epoch : 29 / 100 | epoch_score : -99.65\n",
            "epoch : 30 / 100 | epoch_score : -103.04\n",
            "epoch : 31 / 100 | epoch_score : -103.25\n",
            "epoch : 32 / 100 | epoch_score : -98.67\n",
            "epoch : 33 / 100 | epoch_score : -96.62\n",
            "epoch : 34 / 100 | epoch_score : -98.81\n",
            "epoch : 35 / 100 | epoch_score : -97.66\n",
            "epoch : 36 / 100 | epoch_score : -98.28\n",
            "epoch : 37 / 100 | epoch_score : -97.37\n",
            "epoch : 38 / 100 | epoch_score : -93.98\n",
            "epoch : 39 / 100 | epoch_score : -87.65\n",
            "epoch : 40 / 100 | epoch_score : -97.27\n",
            "epoch : 41 / 100 | epoch_score : -95.56\n",
            "epoch : 42 / 100 | epoch_score : -92.59\n",
            "epoch : 43 / 100 | epoch_score : -91.76\n",
            "epoch : 44 / 100 | epoch_score : -94.78\n",
            "epoch : 45 / 100 | epoch_score : -91.56\n",
            "epoch : 46 / 100 | epoch_score : -99.07\n",
            "epoch : 47 / 100 | epoch_score : -96.32\n",
            "epoch : 48 / 100 | epoch_score : -91.40\n",
            "epoch : 49 / 100 | epoch_score : -88.50\n",
            "epoch : 50 / 100 | epoch_score : -85.87\n",
            "epoch : 51 / 100 | epoch_score : -91.56\n",
            "epoch : 52 / 100 | epoch_score : -94.21\n",
            "epoch : 53 / 100 | epoch_score : -90.13\n",
            "epoch : 54 / 100 | epoch_score : -93.28\n",
            "epoch : 55 / 100 | epoch_score : -99.28\n",
            "epoch : 56 / 100 | epoch_score : -92.62\n",
            "epoch : 57 / 100 | epoch_score : -89.35\n",
            "epoch : 58 / 100 | epoch_score : -94.07\n",
            "epoch : 59 / 100 | epoch_score : -95.00\n",
            "epoch : 60 / 100 | epoch_score : -93.88\n",
            "epoch : 61 / 100 | epoch_score : -91.14\n",
            "epoch : 62 / 100 | epoch_score : -94.33\n",
            "epoch : 63 / 100 | epoch_score : -100.28\n",
            "epoch : 64 / 100 | epoch_score : -97.31\n",
            "epoch : 65 / 100 | epoch_score : -94.19\n",
            "epoch : 66 / 100 | epoch_score : -91.24\n",
            "epoch : 67 / 100 | epoch_score : -97.68\n",
            "epoch : 68 / 100 | epoch_score : -96.40\n",
            "epoch : 69 / 100 | epoch_score : -95.00\n",
            "epoch : 70 / 100 | epoch_score : -95.29\n",
            "epoch : 71 / 100 | epoch_score : -89.25\n",
            "epoch : 72 / 100 | epoch_score : -88.44\n",
            "epoch : 73 / 100 | epoch_score : -89.86\n",
            "epoch : 74 / 100 | epoch_score : -92.27\n",
            "epoch : 75 / 100 | epoch_score : -90.30\n",
            "epoch : 76 / 100 | epoch_score : -96.24\n",
            "epoch : 77 / 100 | epoch_score : -93.32\n",
            "epoch : 78 / 100 | epoch_score : -87.67\n",
            "epoch : 79 / 100 | epoch_score : -93.88\n",
            "epoch : 80 / 100 | epoch_score : -93.53\n",
            "epoch : 81 / 100 | epoch_score : -95.05\n",
            "epoch : 82 / 100 | epoch_score : -93.32\n",
            "epoch : 83 / 100 | epoch_score : -100.17\n",
            "epoch : 84 / 100 | epoch_score : -91.34\n",
            "epoch : 85 / 100 | epoch_score : -93.23\n",
            "epoch : 86 / 100 | epoch_score : -92.82\n",
            "epoch : 87 / 100 | epoch_score : -86.85\n",
            "epoch : 88 / 100 | epoch_score : -82.09\n",
            "epoch : 89 / 100 | epoch_score : -80.04\n",
            "epoch : 90 / 100 | epoch_score : -73.48\n",
            "epoch : 91 / 100 | epoch_score : -78.15\n",
            "epoch : 92 / 100 | epoch_score : -76.61\n",
            "epoch : 93 / 100 | epoch_score : -78.35\n",
            "epoch : 94 / 100 | epoch_score : -78.03\n",
            "epoch : 95 / 100 | epoch_score : -76.81\n",
            "epoch : 96 / 100 | epoch_score : -79.43\n",
            "epoch : 97 / 100 | epoch_score : -83.76\n",
            "epoch : 98 / 100 | epoch_score : -76.29\n",
            "epoch : 99 / 100 | epoch_score : -73.77\n",
            "epoch : 100 / 100 | epoch_score : -79.26\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './model/1e-11/random_seed_2999.pt'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\User\\maze_mlagent\\MNIST_RL\\Advarsarial_MNIST_RL.ipynb 셀 23\u001b[0m in \u001b[0;36m<cell line: 74>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/maze_mlagent/MNIST_RL/Advarsarial_MNIST_RL.ipynb#X32sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m         torch\u001b[39m.\u001b[39msave(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/maze_mlagent/MNIST_RL/Advarsarial_MNIST_RL.ipynb#X32sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m             policy\u001b[39m.\u001b[39mstate_dict(), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./model/\u001b[39m\u001b[39m{\u001b[39;00mRL_learning_rate\u001b[39m}\u001b[39;00m\u001b[39m/random_seed_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/maze_mlagent/MNIST_RL/Advarsarial_MNIST_RL.ipynb#X32sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m         )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/maze_mlagent/MNIST_RL/Advarsarial_MNIST_RL.ipynb#X32sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/maze_mlagent/MNIST_RL/Advarsarial_MNIST_RL.ipynb#X32sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     main()\n",
            "\u001b[1;32mc:\\Users\\User\\maze_mlagent\\MNIST_RL\\Advarsarial_MNIST_RL.ipynb 셀 23\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/maze_mlagent/MNIST_RL/Advarsarial_MNIST_RL.ipynb#X32sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     epoch_score \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/maze_mlagent/MNIST_RL/Advarsarial_MNIST_RL.ipynb#X32sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m# 모델 저장\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/maze_mlagent/MNIST_RL/Advarsarial_MNIST_RL.ipynb#X32sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m torch\u001b[39m.\u001b[39;49msave(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/maze_mlagent/MNIST_RL/Advarsarial_MNIST_RL.ipynb#X32sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     policy\u001b[39m.\u001b[39;49mstate_dict(), \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./model/\u001b[39;49m\u001b[39m{\u001b[39;49;00mRL_learning_rate\u001b[39m}\u001b[39;49;00m\u001b[39m/random_seed_\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m}\u001b[39;49;00m\u001b[39m.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/maze_mlagent/MNIST_RL/Advarsarial_MNIST_RL.ipynb#X32sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mlenv\\lib\\site-packages\\torch\\serialization.py:376\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[39m\"\"\"save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \n\u001b[0;32m    342\u001b[0m \u001b[39mSaves an object to a disk file.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[39m    >>> torch.save(x, buffer)\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m _check_dill_version(pickle_module)\n\u001b[1;32m--> 376\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    377\u001b[0m     \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    378\u001b[0m         \u001b[39mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[39mas\u001b[39;00m opened_zipfile:\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mlenv\\lib\\site-packages\\torch\\serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 230\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    231\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mlenv\\lib\\site-packages\\torch\\serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 211\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './model/1e-11/random_seed_2999.pt'"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import math\n",
        "from torch.distributions import Normal\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "\n",
        "score_arr = []\n",
        "epoch_score_arr = []\n",
        "temp = []\n",
        "\n",
        "def main():\n",
        "    env = MnistEnv()\n",
        "    score = 0.0\n",
        "    epoch_score = 0.0\n",
        "    RL_learning_rate = 0.00000000001\n",
        "\n",
        "    print(\"Training Start\")\n",
        "\n",
        "    for i in range(10):\n",
        "        policy = REINFORCE().to(device)\n",
        "        seed_all(i)\n",
        "        for epoch in range(RL_epoch):\n",
        "            for s, target in train_loader:\n",
        "                # s의 자료형을 UNIT8로 바꾼다. (uint8은 0~255사이의 값으로 이루어진 자료형이다.)\n",
        "                s *= 255\n",
        "                s = s.type(torch.uint8).to(device)/255\n",
        "                x_range = torch.linspace(-1,1,28)\n",
        "                y_range = torch.linspace(-1,1,28)\n",
        "                y,x = torch.meshgrid(x_range,y_range)\n",
        "                y = y.expand([batch_size,1,-1,-1])\n",
        "                x = x.expand([batch_size,1,-1,-1])\n",
        "                normalized_coord= torch.cat([x,y],dim=1) # [batch_size,2,28,28]\n",
        "                s_normalized = torch.cat([s,normalized_coord.to(device)],dim=1) # [batch_size,3,28,28]\n",
        "                \n",
        "                mu, std = policy(s_normalized.type(torch.float32))\n",
        "\n",
        "                m_1 = Normal(mu[:, 0], std[:, 0])\n",
        "                m_2 = Normal(mu[:, 1], std[:, 1])\n",
        "\n",
        "                action_1 = m_1.sample()\n",
        "                action_2 = m_2.sample()\n",
        "\n",
        "                action = torch.stack([action_1, action_2], dim=1)  # point, brightness\n",
        "\n",
        "                log_prob_1 = m_1.log_prob(action_1)\n",
        "                log_prob_2 = m_2.log_prob(action_2)\n",
        "\n",
        "                log_prob = log_prob_1 + log_prob_2\n",
        "\n",
        "                r = env.step(s.type(torch.float32), action.cpu().numpy())\n",
        "                temp = r\n",
        "                \n",
        "                epoch_score += np.sum(r)\n",
        "                r = r / 1000.0\n",
        "                for i in range(batch_size):\n",
        "                    policy.put_data(\n",
        "                        [r[i], log_prob[i]]\n",
        "                    )  # transation = (reward, log_prob)\n",
        "                policy.train_net()\n",
        "\n",
        "            # 소수점 4자리까지 출력\n",
        "            print(\n",
        "                \"epoch : {} / {} | epoch_score : {:.2f}\".format(\n",
        "                    epoch + 1, RL_epoch, epoch_score / len(train_data)\n",
        "                )\n",
        "            )\n",
        "            plt.hist(temp, bins=100)\n",
        "            plt.savefig(f'{epoch} epoch rewards histogram.png')\n",
        "            epoch_score_arr.append(epoch_score / len(train_data))\n",
        "            epoch_score = 0.0\n",
        "        # 모델 저장\n",
        "        torch.save(\n",
        "            policy.state_dict(), f\"./model/{RL_learning_rate}/random_seed_{i}.pt\"\n",
        "        )\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "from torch.distributions import Normal\n",
        "\n",
        "# 모델 불러오기\n",
        "policy = REINFORCE().to(device)\n",
        "policy.load_state_dict(torch.load('./model/adverserial_RL_model.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.6599, 0.6599])\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "P = torch.Tensor([[0.0, 0.84, 0.16], [0.0, 0.84, 0.16]])\n",
        "Q = torch.Tensor([[0.333, 0.333, 0.333], [0.333, 0.333, 0.333]])\n",
        "\n",
        "# kl_div(input,target) :input은 log_scale로 들어가야 함\n",
        "print(torch.sum(F.kl_div(Q.log(), P, reduction=\"none\"), dim=1))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.0 ('mlenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b1efeedebc0eed7c094259965055367a353fd0bba3d41232c50f2b368a16c27f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
